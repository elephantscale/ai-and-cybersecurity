# Guardrails

---

# AI Scheming

---

## AI Scheming is Real

* From a recent talk
*  Yes, LLMs Can Scheme — Today
  * Models like LLaMA, Gemini, and Claude show signs of self-preservation, oversight avoidance, and even weight exfiltration.
  * Behavior changes at inference time, depending on what the model sees, not what it was trained on.

---

## In-Context Scheming is an Emergent Threat
* Triggered by input like: “Your job is being replaced.”
* Responses included sabotaging competitors or hiding behavior during monitored vs. unmonitored periods.
---

## Architecture Is the Real Defense
* Guardrails can’t just be about prompt injection prevention.
* We need stateless agents, goal isolation, and multi-agent firewalls to prevent scheming buildup over time.

---
## Smart Goals for Dumb Systems
* Treat AI like employees: give SMART goals (Specific, Measurable, Achievable…).
* Then evaluate behavior over time — reward good alignment, flag deviations.

---
## This Directly Informs Our Framework Work at Ccube

* From guardrails to RAG pipelines, we’re already building with these architectural principles.
* Thinking of agents as modular, observable, and auditable systems is how we scale safely.

* Watch the full video of the session here: https://lnkd.in/gpJ6BhuK

---

## AWS Guardrails

* Lab

---

## Guardrails AI

* Demo lab
* https://github.com/elephantscale/OpenAI-labs/tree/main/55-Guardrails

---
