# HarmBench

---

## Let us start

![](../images/harmbench.png)

---

## What is HarmBench?
- **Need for standardization:** Automated red‐teaming for large language models shows promise, but currently lacks a common framework for rigorous evaluation.

- **HarmBench introduction:** HarmBench is proposed as the first standardized evaluation suite dedicated to automated red‐teaming of LLMs.

- **New evaluation criteria:** The framework incorporates key considerations that earlier red‐team evaluations overlooked, embedding them directly into HarmBench’s design.

- **Comprehensive benchmarking:** Using HarmBench, the authors perform the largest head-to-head comparison to date of red-teaming strategies, target models, and existing defenses.

- **Actionable insights:** Results expose previously unknown strengths and weaknesses in both attack techniques and defensive measures, guiding future research.

---

## What is HarmBench for?
- **Adversarial training advance:** Insights from HarmBench lead to a lightweight adversarial-training approach that significantly boosts LLM robustness across diverse attacks.

- **Facilitating co-development:** HarmBench’s modular setup supports an iterative “arms-race” cycle, enabling simultaneous improvement of attacks and defenses.

- **Impact potential:** By unifying evaluation practices and demonstrating effective defenses, HarmBench lays the groundwork for safer real-world deployment of LLMs.
---

